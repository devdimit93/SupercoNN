{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2727,
     "status": "ok",
     "timestamp": 1579246474841,
     "user": {
      "displayName": "Dmitry V",
      "photoUrl": "",
      "userId": "04160175683871844591"
     },
     "user_tz": -180
    },
    "id": "wJXbTfoHUPeB",
    "outputId": "2bdefa2b-8ec5-4948-e586-f90b3aadf23b"
   },
   "outputs": [],
   "source": [
    "#init libs\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from tensorflow.python.keras import Input, Model, Sequential, regularizers\n",
    "from tensorflow.python.keras.layers import  Concatenate, Activation, Dropout, Flatten, Dense, LSTM,Conv1D, MaxPooling1D, Reshape, Permute,BatchNormalization\n",
    "from tensorflow.python.keras.utils import plot_model\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras.layers.merge import concatenate\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TGB3vwcUPeL"
   },
   "outputs": [],
   "source": [
    "#open processed datasets\n",
    "#train_dataset has very big size and can not be uploaded to the github\n",
    "#you can create your own dataset by file preparecsvBIG.ipynb or download file from google drive  by link from readme file\n",
    "# train_dataset link https://drive.google.com/open?id=1jiiHWyCiTJSE6kwET9biYVn4FkbiI6XY \n",
    "csv_train=open('processed_data/train_dataset.txt', newline='')\n",
    "reader_train = csv.reader(csv_train, delimiter=',', quotechar='|')\n",
    "\n",
    "csv_test=open('processed_data/test_dataset.txt', newline='')\n",
    "reader_test = csv.reader(csv_test, delimiter=',', quotechar='|')\n",
    "\n",
    "X_train_dataset=[]\n",
    "X_test_dataset=[]\n",
    "\n",
    "Y_train_dataset=[]\n",
    "Y_test_dataset=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUYo9huUBNTA"
   },
   "outputs": [],
   "source": [
    "#control variables for input of neural network\n",
    "\n",
    "\n",
    "target_size=10      #num of position for elements in formula\n",
    "num_elements=17      # paramertrs of one element (1 coefficeent of element in formula, 16 parametrs of element from chemestry table)\n",
    "\n",
    "\n",
    "#you can change target_size, but num_elements must have value 17 for these processed data. \n",
    "# if you want train network with different parametrs, you must change variable \"params_order\" in file preparecsvBIG.ipynb and create your won dataset from source data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhPcW5M1UPeP"
   },
   "outputs": [],
   "source": [
    "#preprocessing datasets\n",
    "train_num_fromulas=0\n",
    "test_num_fromulas=0\n",
    "counter_tr=0    \n",
    "read_it=True\n",
    "while(read_it==True):\n",
    "    try:\n",
    "        counter_tr=counter_tr+1\n",
    "        read_row = next(reader_train)\n",
    "        crit_temp=read_row[-1]\n",
    "        Y_train_dataset.append(float(crit_temp))\n",
    "    \n",
    "        buf=[]\n",
    "        for i in range(len(read_row)-1):\n",
    "            c=read_row[i].split(\" \")\n",
    "            for j in range(len(c)):\n",
    "                c[j]=float(c[j])\n",
    "            c=np.asarray(c)\n",
    "            buf.append(c)\n",
    "        buf=np.asarray(buf)\n",
    "        X_train_dataset.append(buf)\n",
    "        train_num_fromulas=train_num_fromulas+1\n",
    "    except StopIteration:\n",
    "        read_it=False\n",
    "csv_train.close() \n",
    "\n",
    "counter_ts=0\n",
    "read_it=True\n",
    "while(read_it==True):\n",
    "    try:\n",
    "        counter_ts=counter_ts+1\n",
    "        read_row = next(reader_test)\n",
    "        crit_temp=read_row[-1]\n",
    "        Y_test_dataset.append(float(crit_temp))\n",
    "    \n",
    "        buf=[]\n",
    "        for i in range(len(read_row)-1):\n",
    "            c=read_row[i].split(\" \")\n",
    "            for j in range(len(c)):\n",
    "                c[j]=float(c[j])\n",
    "            c=np.asarray(c)\n",
    "            buf.append(c)\n",
    "        buf=np.asarray(buf)\n",
    "        X_test_dataset.append(buf)\n",
    "        test_num_fromulas=test_num_fromulas+1\n",
    "    except StopIteration:\n",
    "        read_it=False\n",
    "csv_test.close() \n",
    "\n",
    "\n",
    "X_test_dataset=np.asarray(X_test_dataset)\n",
    "X_train_dataset=np.asarray(X_train_dataset)\n",
    "\n",
    "add_arr=[]\n",
    "for i in range(num_elements):\n",
    "    add_arr.append(0)\n",
    "add_arr=np.asarray(add_arr)\n",
    "\n",
    "X_test_dataset_processed=[]\n",
    "Y_test_dataset_processed=[]\n",
    "\n",
    "X_val_dataset_processed=[]\n",
    "Y_val_dataset_processed=[]\n",
    "\n",
    "X_train_dataset_processed=[]\n",
    "Y_train_dataset_processed=[]\n",
    "\n",
    "\n",
    "for i in range(X_test_dataset.shape[0]):\n",
    "\n",
    "    c=X_test_dataset[i].shape[0]\n",
    "    dist=target_size-X_test_dataset[i].shape[0]\n",
    "    buf1=[]\n",
    "    \n",
    "    if dist>0:\n",
    "        for j in range(dist):\n",
    "            buf1.append(add_arr)\n",
    "    for j in range(X_test_dataset[i].shape[0]):\n",
    "        buf1.append(X_test_dataset[i][j])\n",
    "    buf1=np.asarray(buf1)\n",
    "    X_test_dataset_processed.append(buf1)\n",
    "    Y_test_dataset_processed.append(Y_test_dataset[i])\n",
    "\n",
    "for i in range(X_train_dataset.shape[0]):\n",
    "\n",
    "    c=X_train_dataset[i].shape[0]\n",
    "    dist=target_size-X_train_dataset[i].shape[0]\n",
    "    buf1=[]\n",
    "    \n",
    "    if dist>0:\n",
    "        for j in range(dist):\n",
    "            buf1.append(add_arr)\n",
    "    for j in range(X_train_dataset[i].shape[0]):\n",
    "        buf1.append(X_train_dataset[i][j])\n",
    "    buf1=np.asarray(buf1)\n",
    "    X_train_dataset_processed.append(buf1)\n",
    "    Y_train_dataset_processed.append(Y_train_dataset[i])\n",
    "\n",
    "\n",
    "X_test_dataset_processed=np.asarray(X_test_dataset_processed)\n",
    "X_train_dataset_processed=np.asarray(X_train_dataset_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13080,
     "status": "ok",
     "timestamp": 1579246486241,
     "user": {
      "displayName": "Dmitry V",
      "photoUrl": "",
      "userId": "04160175683871844591"
     },
     "user_tz": -180
    },
    "id": "O383zE8DUPeT",
    "outputId": "bc672059-f9c0-4679-ed16-62eb9eff7a05"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_shape = (target_size,num_elements)\n",
    "pad='same'\n",
    "\n",
    "kernel_size=3\n",
    "pool_kernel=2\n",
    "kr_size=0.0001\n",
    "model_in = Input(shape=input_shape)\n",
    "perm=Permute((2, 1), input_shape=(target_size, num_elements))(model_in)\n",
    "\n",
    "\n",
    "conv1D010 = Conv1D(64, kernel_size, activation='relu',padding=pad,kernel_regularizer=regularizers.l2(kr_size))(perm)\n",
    "conv1D011 = Conv1D(64, kernel_size ,activation='relu',padding=pad,kernel_regularizer=regularizers.l2(kr_size))(conv1D010)\n",
    "pool011 = MaxPooling1D(pool_size=pool_kernel)(conv1D011)\n",
    "conv1D012 = Conv1D(128, kernel_size, activation='relu',padding=pad,kernel_regularizer=regularizers.l2(kr_size))(pool011)\n",
    "conv1D013 = Conv1D(128, kernel_size ,activation='relu',padding=pad,kernel_regularizer=regularizers.l2(kr_size))(conv1D012)\n",
    "pool012 = MaxPooling1D(pool_size=pool_kernel)(conv1D013)\n",
    "conv1D014 = Conv1D(256, kernel_size ,activation='relu',padding=pad,kernel_regularizer=regularizers.l2(kr_size))(pool012)\n",
    "conv1D015 = Conv1D(256, kernel_size ,activation='relu',padding=pad,kernel_regularizer=regularizers.l2(kr_size))(conv1D014)\n",
    "flatten01=Flatten()(conv1D015)\n",
    "\n",
    "out=Dense(1)(flatten01)\n",
    "model=Model([model_in], out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training parametrs\n",
    "loss='mae'#can be mae or mse\n",
    "optimiser=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "metrics='mae'#can be mae or mse\n",
    "\n",
    "model.compile(optimizer=optimiser, \n",
    "              loss=loss, \n",
    "              metrics=[metrics])\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LSOQeOVX1UV"
   },
   "outputs": [],
   "source": [
    "#function for training early stopping when error<value\n",
    "class EarlyStoppingByLossVal(Callback):\n",
    "    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current < self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2272405,
     "status": "error",
     "timestamp": 1579215062554,
     "user": {
      "displayName": "Dmitry V",
      "photoUrl": "",
      "userId": "04160175683871844591"
     },
     "user_tz": -180
    },
    "id": "TXgU9EwfUPeX",
    "outputId": "98c013c6-61a3-43f1-bfea-c07152f93ead"
   },
   "outputs": [],
   "source": [
    "optimiser=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=optimiser, \n",
    "              loss=loss, \n",
    "              metrics=[metrics])\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(monitor='val_loss', value=4.9, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    ModelCheckpoint(\"checkpoint/conv_C1_checkpoint.ckpt\", monitor='val_loss', save_best_only=True, verbose=0),\n",
    "]\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "history = model.fit(X_train_dataset_processed, \n",
    "                    Y_train_dataset_processed, \n",
    "                    validation_data=(X_test_dataset_processed, Y_test_dataset_processed),\n",
    "                    epochs=300,\n",
    "                    batch_size=200,\n",
    "                    callbacks=callbacks\n",
    "                    )\n",
    "print(history.history.keys())\n",
    "\n",
    "mse, mae = model.evaluate(X_test_dataset_processed, Y_test_dataset_processed, verbose=2)\n",
    "print(\"mae mistake:\", mae)\n",
    "print(\"mistake mse:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykPaUJw9qJ4J"
   },
   "outputs": [],
   "source": [
    "optimiser=optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=optimiser, \n",
    "              loss=loss, \n",
    "              metrics=[metrics])\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(monitor='val_loss', value=4.8, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    ModelCheckpoint(\"checkpoint/conv_C1_checkpoint.ckpt\", monitor='val_loss', save_best_only=True, verbose=0),\n",
    "]\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "history = model.fit(X_train_dataset_processed, \n",
    "                    Y_train_dataset_processed, \n",
    "                    validation_data=(X_test_dataset_processed, Y_test_dataset_processed),\n",
    "                    epochs=300,\n",
    "                    batch_size=200,\n",
    "                    callbacks=callbacks\n",
    "                    )\n",
    "print(history.history.keys())\n",
    "\n",
    "mse, mae = model.evaluate(X_test_dataset_processed, Y_test_dataset_processed, verbose=2)\n",
    "print(\"mae mistake:\", mae)\n",
    "print(\"mistake mse:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWF3WO01ZJ6W"
   },
   "outputs": [],
   "source": [
    "optimiser=optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=optimiser, \n",
    "              loss=loss, \n",
    "              metrics=[metrics])\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(monitor='val_loss', value=4.8, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    ModelCheckpoint(\"checkpoint/conv_C1_checkpoint.ckpt\", monitor='val_loss', save_best_only=True, verbose=0),\n",
    "]\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "history = model.fit(X_train_dataset_processed, \n",
    "                    Y_train_dataset_processed, \n",
    "                    validation_data=(X_test_dataset_processed, Y_test_dataset_processed),\n",
    "                    epochs=300,\n",
    "                    batch_size=200,\n",
    "                    callbacks=callbacks\n",
    "                    )\n",
    "print(history.history.keys())\n",
    "\n",
    "mse, mae = model.evaluate(X_test_dataset_processed, Y_test_dataset_processed, verbose=2)\n",
    "print(\"mae mistake:\", mae)\n",
    "print(\"mistake mse:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVFVdeHwUPeh"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "json_file = open('pretrained_networks/conv_C1_mae' + \".json\", \"w\")\n",
    "#write structure\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "# write wtights\n",
    "model.save_weights('pretrained_networks/conv_C1_mae'+\".h5\")\n",
    "print(\"saving done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88fmvktgsRMW"
   },
   "outputs": [],
   "source": [
    "#you can test it for different temperatures\n",
    "\n",
    "#temperature diapason\n",
    "min_val=90#minimum temperature\n",
    "max_val=999#maximum temperature\n",
    "\n",
    "X_test_0=[]\n",
    "Y_test_0=[]\n",
    "print(\"test\")\n",
    "for i in range(len(Y_test_dataset_processed)):\n",
    "    if Y_test_dataset_processed[i]>min_val and Y_test_dataset_processed[i]<max_val:\n",
    "        X_test_0.append(X_test_dataset_processed[i])\n",
    "        Y_test_0.append(Y_test_dataset_processed[i])\n",
    "X_test_0=np.asarray(X_test_0)\n",
    "pred = model.predict(X_test_0)\n",
    "\n",
    "mistake=0\n",
    "a=[0,0,0,0,0]\n",
    "for i in range(len(Y_test_0)):\n",
    "    mistake=mistake+abs(pred[i][0]-Y_test_0[i])\n",
    "    if abs(pred[i][0]-Y_test_0[i])<1:\n",
    "        a[0]=a[0]+1\n",
    "    if abs(pred[i][0]-Y_test_0[i])>1 and abs(pred[i][0]-Y_test_0[i])<5:\n",
    "        a[1]=a[1]+1\n",
    "    if abs(pred[i][0]-Y_test_0[i])>5 and abs(pred[i][0]-Y_test_0[i])<10:\n",
    "        a[2]=a[2]+1\n",
    "    if abs(pred[i][0]-Y_test_0[i])>10 and abs(pred[i][0]-Y_test_0[i])<100:\n",
    "        a[3]=a[3]+1\n",
    "mistake=mistake/len(Y_test_0)\n",
    "print(\"MAE for these temperatures\")\n",
    "print(mistake)\n",
    "\n",
    "print((a[0]+a[1]+a[2])/((a[0]+a[1]+a[2]+a[3])))\n",
    "print(\"percent of error smaller than 1 degree\")\n",
    "print((a[0])/((a[0]+a[1]+a[2]+a[3])))\n",
    "print(\"percent of error smaller than 5 degree\")\n",
    "print((a[1])/((a[0]+a[1]+a[2]+a[3])))\n",
    "print(\"percent of error smaller than 10 degree\")\n",
    "print((a[2])/((a[0]+a[1]+a[2]+a[3])))\n",
    "\n",
    "print(\"show all results\")\n",
    "for i in range(len(Y_test_0)):\n",
    "    print(\"preficted=\"+str(pred[i][0])+\" real=\"+str(Y_test_0[i]))\n",
    "X_test_0=[]\n",
    "Y_test_0=[]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experiment1V9Valence.ipynb",
   "provenance": [
    {
     "file_id": "1nZLr6cP-eof9vEdJure12mqu9S_Pc0sV",
     "timestamp": 1578647683632
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
