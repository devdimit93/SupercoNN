{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2727,
     "status": "ok",
     "timestamp": 1579246474841,
     "user": {
      "displayName": "Dmitry V",
      "photoUrl": "",
      "userId": "04160175683871844591"
     },
     "user_tz": -180
    },
    "id": "wJXbTfoHUPeB",
    "outputId": "2bdefa2b-8ec5-4948-e586-f90b3aadf23b"
   },
   "outputs": [],
   "source": [
    "#init libs\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from tensorflow.python.keras import Input, Model, Sequential\n",
    "from tensorflow.python.keras.layers import  Concatenate, Activation, Dropout, Flatten, Dense, LSTM,Conv1D, MaxPooling1D, Reshape, Permute,BatchNormalization\n",
    "from tensorflow.python.keras.utils import plot_model\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras.layers.merge import concatenate\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TGB3vwcUPeL"
   },
   "outputs": [],
   "source": [
    "#open processed datasets\n",
    "csv_train=open('processed_data/train_dataset.txt', newline='')\n",
    "reader_train = csv.reader(csv_train, delimiter=',', quotechar='|')\n",
    "\n",
    "csv_test=open('processed_data/test_dataset.txt', newline='')\n",
    "reader_test = csv.reader(csv_test, delimiter=',', quotechar='|')\n",
    "\n",
    "X_train_dataset=[]\n",
    "X_test_dataset=[]\n",
    "\n",
    "Y_train_dataset=[]\n",
    "Y_test_dataset=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUYo9huUBNTA"
   },
   "outputs": [],
   "source": [
    "#control variables for input of neural network\n",
    "\n",
    "\n",
    "target_size=10      #num of position for elements in formula\n",
    "num_elements=17      # paramertrs of one element (1 coefficeent of element in formula, 16 parametrs of element from chemestry table)\n",
    "\n",
    "\n",
    "#you can change target_size, but num_elements must have value 17 for these processed data. \n",
    "# if you want train network with different parametrs, you must change variable \"params_order\" in file preparecsvBIG.ipynb and create your won dataset from source data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhPcW5M1UPeP"
   },
   "outputs": [],
   "source": [
    "#preprocessing datasets\n",
    "train_num_fromulas=0\n",
    "test_num_fromulas=0\n",
    "counter_tr=0    \n",
    "read_it=True\n",
    "while(read_it==True):\n",
    "    try:\n",
    "        counter_tr=counter_tr+1\n",
    "        read_row = next(reader_train)\n",
    "        crit_temp=read_row[-1]\n",
    "        Y_train_dataset.append(float(crit_temp))\n",
    "    \n",
    "        buf=[]\n",
    "        for i in range(len(read_row)-1):\n",
    "            c=read_row[i].split(\" \")\n",
    "            for j in range(len(c)):\n",
    "                c[j]=float(c[j])\n",
    "            c=np.asarray(c)\n",
    "            buf.append(c)\n",
    "        buf=np.asarray(buf)\n",
    "        X_train_dataset.append(buf)\n",
    "        train_num_fromulas=train_num_fromulas+1\n",
    "    except StopIteration:\n",
    "        read_it=False\n",
    "csv_train.close() \n",
    "\n",
    "counter_ts=0\n",
    "read_it=True\n",
    "while(read_it==True):\n",
    "    try:\n",
    "        counter_ts=counter_ts+1\n",
    "        read_row = next(reader_test)\n",
    "        crit_temp=read_row[-1]\n",
    "        Y_test_dataset.append(float(crit_temp))\n",
    "    \n",
    "        buf=[]\n",
    "        for i in range(len(read_row)-1):\n",
    "            c=read_row[i].split(\" \")\n",
    "            for j in range(len(c)):\n",
    "                c[j]=float(c[j])\n",
    "            c=np.asarray(c)\n",
    "            buf.append(c)\n",
    "        buf=np.asarray(buf)\n",
    "        X_test_dataset.append(buf)\n",
    "        test_num_fromulas=test_num_fromulas+1\n",
    "    except StopIteration:\n",
    "        read_it=False\n",
    "csv_test.close() \n",
    "\n",
    "\n",
    "X_test_dataset=np.asarray(X_test_dataset)\n",
    "X_train_dataset=np.asarray(X_train_dataset)\n",
    "\n",
    "add_arr=[]\n",
    "for i in range(num_elements):\n",
    "    add_arr.append(0)\n",
    "add_arr=np.asarray(add_arr)\n",
    "\n",
    "X_test_dataset_processed=[]\n",
    "Y_test_dataset_processed=[]\n",
    "\n",
    "X_val_dataset_processed=[]\n",
    "Y_val_dataset_processed=[]\n",
    "\n",
    "X_train_dataset_processed=[]\n",
    "Y_train_dataset_processed=[]\n",
    "\n",
    "\n",
    "for i in range(X_test_dataset.shape[0]):\n",
    "\n",
    "    c=X_test_dataset[i].shape[0]\n",
    "    dist=target_size-X_test_dataset[i].shape[0]\n",
    "    buf1=[]\n",
    "    \n",
    "    if dist>0:\n",
    "        for j in range(dist):\n",
    "            buf1.append(add_arr)\n",
    "    for j in range(X_test_dataset[i].shape[0]):\n",
    "        buf1.append(X_test_dataset[i][j])\n",
    "    buf1=np.asarray(buf1)\n",
    "    X_test_dataset_processed.append(buf1)\n",
    "    Y_test_dataset_processed.append(Y_test_dataset[i])\n",
    "\n",
    "for i in range(X_train_dataset.shape[0]):\n",
    "\n",
    "    c=X_train_dataset[i].shape[0]\n",
    "    dist=target_size-X_train_dataset[i].shape[0]\n",
    "    buf1=[]\n",
    "    \n",
    "    if dist>0:\n",
    "        for j in range(dist):\n",
    "            buf1.append(add_arr)\n",
    "    for j in range(X_train_dataset[i].shape[0]):\n",
    "        buf1.append(X_train_dataset[i][j])\n",
    "    buf1=np.asarray(buf1)\n",
    "    X_train_dataset_processed.append(buf1)\n",
    "    Y_train_dataset_processed.append(Y_train_dataset[i])\n",
    "\n",
    "\n",
    "X_test_dataset_processed=np.asarray(X_test_dataset_processed)\n",
    "X_train_dataset_processed=np.asarray(X_train_dataset_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13080,
     "status": "ok",
     "timestamp": 1579246486241,
     "user": {
      "displayName": "Dmitry V",
      "photoUrl": "",
      "userId": "04160175683871844591"
     },
     "user_tz": -180
    },
    "id": "O383zE8DUPeT",
    "outputId": "bc672059-f9c0-4679-ed16-62eb9eff7a05"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_shape = (target_size,num_elements)\n",
    "pad='same'\n",
    "\n",
    "kernel_size=3\n",
    "pool_kernel=2\n",
    "\n",
    "model_in = Input(shape=input_shape)\n",
    "\n",
    "perm=Permute((2, 1), input_shape=(target_size, num_elements))(model_in)\n",
    "conv1D010 = Conv1D(32, kernel_size, activation='relu',padding=pad)(perm)\n",
    "conv1D011 = Conv1D(32, kernel_size ,activation='relu',padding=pad)(conv1D010)\n",
    "pool011 = MaxPooling1D(pool_size=pool_kernel)(conv1D011)\n",
    "conv1D012 = Conv1D(64, kernel_size, activation='relu',padding=pad)(pool011)\n",
    "conv1D013 = Conv1D(64, kernel_size ,activation='relu',padding=pad)(conv1D012)\n",
    "pool012 = MaxPooling1D(pool_size=pool_kernel)(conv1D013)\n",
    "conv1D014 = Conv1D(128, kernel_size ,activation='relu',padding=pad)(pool012)\n",
    "conv1D015 = Conv1D(128, kernel_size ,activation='relu',padding=pad)(conv1D014)\n",
    "flatten01=Flatten()(conv1D015)\n",
    "\n",
    "out=Dense(1)(flatten01)\n",
    "model=Model([model_in], out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training parametrs\n",
    "loss='mae'#can be mae or mse\n",
    "optimiser=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "metrics='mae'#can be mae or mse\n",
    "\n",
    "model.compile(optimizer=optimiser, \n",
    "              loss=loss, \n",
    "              metrics=[metrics])\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LSOQeOVX1UV"
   },
   "outputs": [],
   "source": [
    "#function for training early stopping when error<value\n",
    "class EarlyStoppingByLossVal(Callback):\n",
    "    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current < self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2272405,
     "status": "error",
     "timestamp": 1579215062554,
     "user": {
      "displayName": "Dmitry V",
      "photoUrl": "",
      "userId": "04160175683871844591"
     },
     "user_tz": -180
    },
    "id": "TXgU9EwfUPeX",
    "outputId": "98c013c6-61a3-43f1-bfea-c07152f93ead"
   },
   "outputs": [],
   "source": [
    "optimiser=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=optimiser, \n",
    "              loss=loss, \n",
    "              metrics=[metrics])\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(monitor='val_loss', value=4.9, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    ModelCheckpoint(\"checkpoint/conv_C3_checkpoint.ckpt\", monitor='val_loss', save_best_only=True, verbose=0),\n",
    "]\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "history = model.fit(X_train_dataset_processed, \n",
    "                    Y_train_dataset_processed, \n",
    "                    validation_data=(X_test_dataset_processed, Y_test_dataset_processed),\n",
    "                    epochs=300,\n",
    "                    batch_size=200,\n",
    "                    callbacks=callbacks\n",
    "                    )\n",
    "print(history.history.keys())\n",
    "\n",
    "mse, mae = model.evaluate(X_test_dataset_processed, Y_test_dataset_processed, verbose=2)\n",
    "print(\"mae mistake:\", mae)\n",
    "print(\"mistake mse:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykPaUJw9qJ4J"
   },
   "outputs": [],
   "source": [
    "optimiser=optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=optimiser, \n",
    "              loss=loss, \n",
    "              metrics=[metrics])\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(monitor='val_loss', value=4.8, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    ModelCheckpoint(\"checkpoint/conv_C3_checkpoint.ckpt\", monitor='val_loss', save_best_only=True, verbose=0),\n",
    "]\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "history = model.fit(X_train_dataset_processed, \n",
    "                    Y_train_dataset_processed, \n",
    "                    validation_data=(X_test_dataset_processed, Y_test_dataset_processed),\n",
    "                    epochs=300,\n",
    "                    batch_size=200,\n",
    "                    callbacks=callbacks\n",
    "                    )\n",
    "print(history.history.keys())\n",
    "\n",
    "mse, mae = model.evaluate(X_test_dataset_processed, Y_test_dataset_processed, verbose=2)\n",
    "print(\"mae mistake:\", mae)\n",
    "print(\"mistake mse:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWF3WO01ZJ6W"
   },
   "outputs": [],
   "source": [
    "optimiser=optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=optimiser, \n",
    "              loss=loss, \n",
    "              metrics=[metrics])\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(monitor='val_loss', value=4.8, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    ModelCheckpoint(\"checkpoint/conv_C3_checkpoint.ckpt\", monitor='val_loss', save_best_only=True, verbose=0),\n",
    "]\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "history = model.fit(X_train_dataset_processed, \n",
    "                    Y_train_dataset_processed, \n",
    "                    validation_data=(X_test_dataset_processed, Y_test_dataset_processed),\n",
    "                    epochs=300,\n",
    "                    batch_size=200,\n",
    "                    callbacks=callbacks\n",
    "                    )\n",
    "print(history.history.keys())\n",
    "\n",
    "mse, mae = model.evaluate(X_test_dataset_processed, Y_test_dataset_processed, verbose=2)\n",
    "print(\"mae mistake:\", mae)\n",
    "print(\"mistake mse:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVFVdeHwUPeh"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "json_file = open('pretrained_networks/conv_C3' + \".json\", \"w\")\n",
    "#write structure\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "# write wtights\n",
    "model.save_weights('pretrained_networks/conv_C3'+\".h5\")\n",
    "print(\"saving done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88fmvktgsRMW"
   },
   "outputs": [],
   "source": [
    "#you can test it for different temperatures\n",
    "\n",
    "#temperature diapason\n",
    "min_val=90#minimum temperature\n",
    "max_val=999#maximum temperature\n",
    "\n",
    "X_test_0=[]\n",
    "Y_test_0=[]\n",
    "print(\"test\")\n",
    "for i in range(len(Y_test_dataset_processed)):\n",
    "    if Y_test_dataset_processed[i]>min_val and Y_test_dataset_processed[i]<max_val:\n",
    "        X_test_0.append(X_test_dataset_processed[i])\n",
    "        Y_test_0.append(Y_test_dataset_processed[i])\n",
    "X_test_0=np.asarray(X_test_0)\n",
    "pred = model.predict(X_test_0)\n",
    "\n",
    "mistake=0\n",
    "a=[0,0,0,0,0]\n",
    "for i in range(len(Y_test_0)):\n",
    "    mistake=mistake+abs(pred[i][0]-Y_test_0[i])\n",
    "    if abs(pred[i][0]-Y_test_0[i])<1:\n",
    "        a[0]=a[0]+1\n",
    "    if abs(pred[i][0]-Y_test_0[i])>1 and abs(pred[i][0]-Y_test_0[i])<5:\n",
    "        a[1]=a[1]+1\n",
    "    if abs(pred[i][0]-Y_test_0[i])>5 and abs(pred[i][0]-Y_test_0[i])<10:\n",
    "        a[2]=a[2]+1\n",
    "    if abs(pred[i][0]-Y_test_0[i])>10 and abs(pred[i][0]-Y_test_0[i])<100:\n",
    "        a[3]=a[3]+1\n",
    "mistake=mistake/len(Y_test_0)\n",
    "print(\"MAE for these temperatures\")\n",
    "print(mistake)\n",
    "\n",
    "print((a[0]+a[1]+a[2])/((a[0]+a[1]+a[2]+a[3])))\n",
    "print(\"percent of error smaller than 1 degree\")\n",
    "print((a[0])/((a[0]+a[1]+a[2]+a[3])))\n",
    "print(\"percent of error smaller than 5 degree\")\n",
    "print((a[1])/((a[0]+a[1]+a[2]+a[3])))\n",
    "print(\"percent of error smaller than 10 degree\")\n",
    "print((a[2])/((a[0]+a[1]+a[2]+a[3])))\n",
    "\n",
    "print(\"show all results\")\n",
    "for i in range(len(Y_test_0)):\n",
    "    print(\"preficted=\"+str(pred[i][0])+\" real=\"+str(Y_test_0[i]))\n",
    "X_test_0=[]\n",
    "Y_test_0=[]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experiment1V9Valence.ipynb",
   "provenance": [
    {
     "file_id": "1nZLr6cP-eof9vEdJure12mqu9S_Pc0sV",
     "timestamp": 1578647683632
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
